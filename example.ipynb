{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4e8e0cb-0e4d-4d16-bc3a-8a0f4188b001",
   "metadata": {},
   "source": [
    "# PyTorch DDP Speech Recognition Training Example\n",
    "\n",
    "This example demonstrates how to train a transformer network to classify audio words with Google's [Speech Command](https://huggingface.co/datasets/google/speech_commands). It's a very small dataset that contains words for classification. The dataset is small(2.3G) and it's quite fast to train a small model.\n",
    "\n",
    "This notebook walks you through running that example locally, and how to easily scale PyTorch DDP across multiple nodes with Kubeflow TrainJob."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642915a6",
   "metadata": {},
   "source": [
    "\n",
    "## Prepare the Kubernetes environment using Kind\n",
    "\n",
    "If you already have your own Kubernetes cluster, you can skip this step.\n",
    "\n",
    "For demo purpose, we will create a k8s cluster with [Kind](https://kind.sigs.k8s.io/). In the same folder of this example Jupyter notebook file, there is a Kind file called `kind-config.yaml`. It will create a k8s cluster with 3 workers and /data from host server is mounted to kind k8s cluster server. Therefore you can download data to /data in your local machine and can be accessed from kind cluster as well.\n",
    "\n",
    "To create the kind cluster, run the following command:\n",
    "**Notice** This will create a Kind cluster named 'ml', you only need to run this command once. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7482f7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating cluster \"ml\" ...\n",
      " \u001b[32m✓\u001b[0m Ensuring node image (kindest/node:v1.34.0) 🖼\n",
      " \u001b[32m✓\u001b[0m Preparing nodes 📦 📦 📦 📦 7l\n",
      " \u001b[32m✓\u001b[0m Writing configuration 📜7l\n",
      " \u001b[32m✓\u001b[0m Starting control-plane 🕹️7l\n",
      " \u001b[32m✓\u001b[0m Installing CNI 🔌7l\n",
      " \u001b[32m✓\u001b[0m Installing StorageClass 💾7l\n",
      " \u001b[32m✓\u001b[0m Joining worker nodes 🚜7l\n",
      "Set kubectl context to \"kind-ml\"\n",
      "You can now use your cluster with:\n",
      "\n",
      "kubectl cluster-info --context kind-ml\n",
      "\n",
      "Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community 🙂\n"
     ]
    }
   ],
   "source": [
    "!kind create cluster --name ml --config kind-config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d3b0a",
   "metadata": {},
   "source": [
    "## Add CRD and Kubeflow Trainer operator to Kubernetes cluster\n",
    "\n",
    "The full instruction is at [here](https://www.kubeflow.org/docs/components/trainer/operator-guides/installation/). In short, run this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58964e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace/kubeflow-system serverside-applied\n",
      "\u001b[33;1mWarning:\u001b[0m unrecognized format \"int32\"\n",
      "\u001b[33;1mWarning:\u001b[0m unrecognized format \"int64\"\n",
      "customresourcedefinition.apiextensions.k8s.io/clustertrainingruntimes.trainer.kubeflow.org serverside-applied\n",
      "customresourcedefinition.apiextensions.k8s.io/jobsets.jobset.x-k8s.io serverside-applied\n",
      "customresourcedefinition.apiextensions.k8s.io/trainingruntimes.trainer.kubeflow.org serverside-applied\n",
      "customresourcedefinition.apiextensions.k8s.io/trainjobs.trainer.kubeflow.org serverside-applied\n",
      "serviceaccount/jobset-controller-manager serverside-applied\n",
      "serviceaccount/kubeflow-trainer-controller-manager serverside-applied\n",
      "role.rbac.authorization.k8s.io/jobset-leader-election-role serverside-applied\n",
      "clusterrole.rbac.authorization.k8s.io/jobset-manager-role serverside-applied\n",
      "clusterrole.rbac.authorization.k8s.io/jobset-metrics-reader serverside-applied\n",
      "clusterrole.rbac.authorization.k8s.io/jobset-proxy-role serverside-applied\n",
      "clusterrole.rbac.authorization.k8s.io/kubeflow-trainer-controller-manager serverside-applied\n",
      "rolebinding.rbac.authorization.k8s.io/jobset-leader-election-rolebinding serverside-applied\n",
      "clusterrolebinding.rbac.authorization.k8s.io/jobset-manager-rolebinding serverside-applied\n",
      "clusterrolebinding.rbac.authorization.k8s.io/jobset-metrics-reader-rolebinding serverside-applied\n",
      "clusterrolebinding.rbac.authorization.k8s.io/jobset-proxy-rolebinding serverside-applied\n",
      "clusterrolebinding.rbac.authorization.k8s.io/kubeflow-trainer-controller-manager serverside-applied\n",
      "configmap/jobset-manager-config serverside-applied\n",
      "secret/jobset-webhook-server-cert serverside-applied\n",
      "secret/kubeflow-trainer-webhook-cert serverside-applied\n",
      "service/jobset-controller-manager-metrics-service serverside-applied\n",
      "service/jobset-webhook-service serverside-applied\n",
      "service/kubeflow-trainer-controller-manager serverside-applied\n",
      "deployment.apps/jobset-controller-manager serverside-applied\n",
      "deployment.apps/kubeflow-trainer-controller-manager serverside-applied\n",
      "mutatingwebhookconfiguration.admissionregistration.k8s.io/jobset-mutating-webhook-configuration serverside-applied\n",
      "validatingwebhookconfiguration.admissionregistration.k8s.io/jobset-validating-webhook-configuration serverside-applied\n",
      "validatingwebhookconfiguration.admissionregistration.k8s.io/validator.trainer.kubeflow.org serverside-applied\n"
     ]
    }
   ],
   "source": [
    "!export VERSION=v2.0.0\n",
    "!kubectl apply --server-side -k \"https://github.com/kubeflow/trainer.git/manifests/overlays/manager?ref=${VERSION}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb78ca0",
   "metadata": {},
   "source": [
    "## Prepare Docker Image\n",
    "\n",
    "We need to create a Docker image with requirements.txt, the `Dockerfile` and `requirements.txt` can be found at the same folder of this Jupyter Notebook.\n",
    "\n",
    "To build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0612281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                          docker:default\n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (1/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 197B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.8.0-cuda12.8  0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (1/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 197B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.8.0-cuda12.8  0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (1/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 197B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.8.0-cuda12.8  0.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (1/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 197B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.8.0-cuda12.8  0.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.8s (2/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 197B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/pytorch:2.8.0-cuda12.8  0.7s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.9s (9/9)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 197B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/pytorch:2.8.0-cuda12.8  0.7s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 38B                                           0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/pytorch/pytorch:2.8.0-cuda12.8-cudnn9-devel@sha2  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] RUN apt-get update && apt-get install -y ffmpeg           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/4] COPY requirements.txt .                                   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/4] RUN pip install -r requirements.txt                       0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:f98d06d275aa85a352ca3b4ee886fd7c10a052fef62e4  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/speech-recognition-image:0.1            0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.0s (9/9) FINISHED                                 docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 197B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/pytorch:2.8.0-cuda12.8  0.7s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 38B                                           0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/pytorch/pytorch:2.8.0-cuda12.8-cudnn9-devel@sha2  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] RUN apt-get update && apt-get install -y ffmpeg           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/4] COPY requirements.txt .                                   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/4] RUN pip install -r requirements.txt                       0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:f98d06d275aa85a352ca3b4ee886fd7c10a052fef62e4  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/speech-recognition-image:0.1            0.0s\n",
      "\u001b[0m\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!docker build -t speech-recognition-image:0.1 -f Dockerfile ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0127318f",
   "metadata": {},
   "source": [
    "### Load image to Kind cluster\n",
    "\n",
    "#### Kind cluster\n",
    "\n",
    "If you are using a local Kind cluster, run the following command to load docker image to your local cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dba24342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: \"speech-recognition-image:0.1\" with ID \"sha256:f98d06d275aa85a352ca3b4ee886fd7c10a052fef62e430c01853e6a1cffc689\" not yet present on node \"ml-worker2\", loading...\n",
      "Image: \"speech-recognition-image:0.1\" with ID \"sha256:f98d06d275aa85a352ca3b4ee886fd7c10a052fef62e430c01853e6a1cffc689\" not yet present on node \"ml-worker\", loading...\n",
      "Image: \"speech-recognition-image:0.1\" with ID \"sha256:f98d06d275aa85a352ca3b4ee886fd7c10a052fef62e430c01853e6a1cffc689\" not yet present on node \"ml-worker3\", loading...\n",
      "Image: \"speech-recognition-image:0.1\" with ID \"sha256:f98d06d275aa85a352ca3b4ee886fd7c10a052fef62e430c01853e6a1cffc689\" not yet present on node \"ml-control-plane\", loading...\n"
     ]
    }
   ],
   "source": [
    "!kind load docker-image speech-recognition-image:0.1 --name ml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3725b970",
   "metadata": {},
   "source": [
    "#### Kubernetes cluster\n",
    "\n",
    "If you are not using the local Kind cluster for testing. Please upload the code to your own Docker registry.\n",
    "\n",
    "```bash\n",
    "docker image push <your docker image name with registry info>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d6be3a",
   "metadata": {},
   "source": [
    "## Add Runtime to K8s cluster\n",
    "\n",
    "In the same folder of this Jypyter notebook file, there is a `kubeflow-runtime-example.yaml`. \n",
    "\n",
    "**Please modify the image to the one you just uploaded.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "582d5a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustertrainingruntime.trainer.kubeflow.org/torch-distributed-speech-recognition created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f kubeflow-runtime-example.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2503c397",
   "metadata": {},
   "source": [
    "## Install the Kubeflow SDK\n",
    "\n",
    "You need to install the Kubeflow SDK to interact with Kubeflow Trainer APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f98b1618-bf56-4c23-96d6-1e8332933ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/kubeflow/sdk.git@main\n",
      "  Cloning https://github.com/kubeflow/sdk.git (to revision main) to /tmp/pip-req-build-dhm1y012\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/kubeflow/sdk.git /tmp/pip-req-build-dhm1y012\n",
      "  Resolved https://github.com/kubeflow/sdk.git to commit 6709dcff0f3e68d44b37531d3154829e626f4b62\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: kubeflow-trainer-api>=2.0.0 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from kubeflow==0.1.0) (2.0.0)\n",
      "Requirement already satisfied: kubernetes>=27.2.0 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from kubeflow==0.1.0) (33.1.0)\n",
      "Requirement already satisfied: pydantic>=2.10.0 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from kubeflow==0.1.0) (2.11.7)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow==0.1.0) (2025.8.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow==0.1.0) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow==0.1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow==0.1.0) (6.0.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow==0.1.0) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow==0.1.0) (1.8.0)\n",
      "Requirement already satisfied: requests in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow==0.1.0) (2.32.5)\n",
      "Requirement already satisfied: requests-oauthlib in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow==0.1.0) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow==0.1.0) (3.3.1)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow==0.1.0) (2.5.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow==0.1.0) (0.10)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=27.2.0->kubeflow==0.1.0) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=27.2.0->kubeflow==0.1.0) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=27.2.0->kubeflow==0.1.0) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=27.2.0->kubeflow==0.1.0) (0.6.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from pydantic>=2.10.0->kubeflow==0.1.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from pydantic>=2.10.0->kubeflow==0.1.0) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from pydantic>=2.10.0->kubeflow==0.1.0) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from pydantic>=2.10.0->kubeflow==0.1.0) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from requests->kubernetes>=27.2.0->kubeflow==0.1.0) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from requests->kubernetes>=27.2.0->kubeflow==0.1.0) (3.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/kubeflow/sdk.git@main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105147ca-c093-457f-99ca-e72f4c0dce93",
   "metadata": {},
   "source": [
    "## Prepare Speech Command Dataset\n",
    "\n",
    "For demo purpose and to simply the process, we are downloading data to /data in the host server. And in the Kind cluster, it's mounting the host's /data folder to cluster's server's /data folder. And in the Kubeflow Runtime, it's mounting the data with hostpath on /data. Therefore everyone is accessing data in /data folder. **Please make sure there is /data folder in the host server.**\n",
    "\n",
    "For other clusters, please create a volume to make sure data can be accessed via /data.\n",
    "\n",
    "The exact path for Speech Command dataset is `/data/SpeechCommands/speech_commands_v0.02`.\n",
    "\n",
    "To download data, run the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ad4906a-415f-4f03-88b4-89c4e84d519c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading SpeechCommands dataset...\n",
      "Download complete!\n",
      "Number of training samples: 105829\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "\n",
    "print(\"Downloading SpeechCommands dataset...\")\n",
    "\n",
    "# This command will download the data to a folder named \"SpeechCommands\"\n",
    "# in your current directory if it's not already there.\n",
    "train_dataset = torchaudio.datasets.SPEECHCOMMANDS(root=\"/data\", download=True)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3985ae5-15f6-4c4c-bdc7-828411450dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_background_noise_  five     left     README.md\t\ttree\n",
      "backward\t    follow   LICENSE  right\t\ttwo\n",
      "bed\t\t    forward  marvin   seven\t\tup\n",
      "bird\t\t    four     nine     sheila\t\tvalidation_list.txt\n",
      "cat\t\t    go\t     no       six\t\tvisual\n",
      "dog\t\t    happy    off      stop\t\twow\n",
      "down\t\t    house    on       testing_list.txt\tyes\n",
      "eight\t\t    learn    one      three\t\tzero\n"
     ]
    }
   ],
   "source": [
    "!ls /data/SpeechCommands/speech_commands_v0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9ea4c6-40db-4bd1-bae9-e9d5d074d7ac",
   "metadata": {},
   "source": [
    "## Create TrainerClient with Kubeflow Trainer SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89bee197-0a0e-487b-8512-45c4a9ef68f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.trainer import CustomTrainer, TrainerClient\n",
    "\n",
    "client = TrainerClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b3098d-a90b-46a9-936b-882aee69c049",
   "metadata": {},
   "source": [
    "## Get runtime from K8s cluster\n",
    "\n",
    "After running the below cell, you should see something like the below. If the following cell shows nothing, it mostly because the Custom Kubeflow Runtime is not created. Please go back to previous step to create Kubeflow Runtime with `kubectl`.\n",
    "```\n",
    "Runtime(name='torch-distributed-speech-recognition', trainer=RuntimeTrainer(trainer_type=<TrainerType.CUSTOM_TRAINER: 'CustomTrainer'>, framework='torch', num_nodes=1, device='Unknown', device_count='Unknown'), pretrained_model=None)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "778f9264-9ca6-43f9-afb0-fa00f353f072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime(name='torch-distributed-speech-recognition', trainer=RuntimeTrainer(trainer_type=<TrainerType.CUSTOM_TRAINER: 'CustomTrainer'>, framework='torch', num_nodes=1, device='Unknown', device_count='Unknown'), pretrained_model=None)\n"
     ]
    }
   ],
   "source": [
    "for runtime in client.list_runtimes():\n",
    "    print(runtime)\n",
    "    if runtime.name == \"torch-distributed-speech-recognition\":\n",
    "        torch_runtime = runtime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3853b4e-a97c-4b91-bf73-edb9a0d22508",
   "metadata": {},
   "source": [
    "## Start training\n",
    "\n",
    "The training code is in the `train_with_kubeflow_trainer.py`, which is in the same folder of current Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f072ad6b-464c-4112-a0ec-a91e6ee2c143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # 1. IMPORTS\n",
    "    # ---\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    import torchaudio\n",
    "    import os  # To navigate file paths\n",
    "    import json\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    import random\n",
    "    from datetime import datetime\n",
    "    import torch.distributed as dist\n",
    "    from torch.utils.data.distributed import DistributedSampler\n",
    "    from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "    debug = False\n",
    "\n",
    "    # Its job: Load an audio file, convert it to a spectrogram, and return it with its numerical label.\n",
    "    def load_data(data_path):\n",
    "        audio_info = []\n",
    "        label_map = {}\n",
    "        label_map_reverse = {}\n",
    "        # Walk through the data directory to find all audio files\n",
    "        # full_data_path = os.path.join(os.path.dirname(__file__), data_path)\n",
    "        full_data_path = data_path\n",
    "\n",
    "        # Get all subdirectories (word labels), excluding special directories\n",
    "        labels = []\n",
    "        for item in os.listdir(full_data_path):\n",
    "            item_path = os.path.join(full_data_path, item)\n",
    "            if (\n",
    "                os.path.isdir(item_path)\n",
    "                and not item.startswith(\"_\")\n",
    "                and item != \"LICENSE\"\n",
    "            ):\n",
    "                labels.append(item)\n",
    "\n",
    "        # Sort labels for consistent mapping\n",
    "        labels.sort()\n",
    "\n",
    "        # Create label to integer mapping\n",
    "        label_map = {label: idx for idx, label in enumerate(labels)}\n",
    "        label_map_reverse = {idx: label for label, idx in enumerate(labels)}\n",
    "\n",
    "        # Collect all audio files\n",
    "        for label in labels:\n",
    "            label_dir = os.path.join(full_data_path, label)\n",
    "            for filename in os.listdir(label_dir):\n",
    "                if filename.endswith(\".wav\"):\n",
    "                    # Store relative path from voice-recognition folder\n",
    "                    relative_path = os.path.join(data_path, label, filename)\n",
    "                    audio_info.append({\"filename\": relative_path, \"label\": label})\n",
    "        return audio_info, label_map, label_map_reverse\n",
    "\n",
    "    def split_data(audio_info):\n",
    "        # Don't shuffle here - let DistributedSampler handle shuffling\n",
    "        # This ensures proper distributed sampling\n",
    "        random.seed(41)\n",
    "        random.shuffle(audio_info)\n",
    "        print(f\"audio info length: {len(audio_info)}\")\n",
    "        train_size = int(len(audio_info) * 0.95)\n",
    "        val_size = int(len(audio_info) * 0.03)\n",
    "        test_size = len(audio_info) - train_size - val_size\n",
    "        print(f\"train size: {train_size}, val size: {val_size}, test size: {test_size}\")\n",
    "\n",
    "        audio_info_training = audio_info[:train_size]\n",
    "        audio_info_validation = audio_info[train_size : train_size + val_size]\n",
    "        audio_info_test = audio_info[train_size + val_size :]\n",
    "        return audio_info_training, audio_info_validation, audio_info_test\n",
    "\n",
    "    class SpeechCommandsDataset(Dataset):\n",
    "        def __init__(\n",
    "            self, audio_info, label_map, label_map_reverse, data_path_prefix=None\n",
    "        ):\n",
    "            self.data_path_prefix = data_path_prefix\n",
    "            self.audio_info = audio_info\n",
    "            self.label_map = label_map\n",
    "            self.label_map_reverse = label_map_reverse\n",
    "            self.transform = torchaudio.transforms.MelSpectrogram(n_mels=128)\n",
    "\n",
    "            print(json.dumps(self.audio_info[0:3], indent=4))\n",
    "            print(self.label_map)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.audio_info)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            audio_info = self.audio_info[index]\n",
    "\n",
    "            # Build full path to audio file\n",
    "            if self.data_path_prefix is None:\n",
    "                audio_path = os.path.join(\n",
    "                    os.path.dirname(__file__), audio_info[\"filename\"]\n",
    "                )\n",
    "            else:\n",
    "                audio_path = os.path.join(self.data_path_prefix, audio_info[\"filename\"])\n",
    "\n",
    "            # Load the audio file\n",
    "            waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "            # Transform to spectrogram\n",
    "            spectrogram = self.transform(waveform)\n",
    "            # print(f\"Spectrogram shape: {spectrogram.shape}\")\n",
    "\n",
    "            # Get the numerical label from the word label\n",
    "            label = self.label_map[audio_info[\"label\"]]\n",
    "\n",
    "            return spectrogram, label\n",
    "\n",
    "    def collate_fn_spectrogram(batch):\n",
    "        # batch is a list of tuples (spectrogram, label)\n",
    "\n",
    "        # Let's set our target length\n",
    "        target_length = 81\n",
    "\n",
    "        spectrograms = []\n",
    "        labels = []\n",
    "\n",
    "        # Loop through each item in the batch\n",
    "        for spec, label in batch:\n",
    "            # spec shape is (1, num_features, time)\n",
    "            current_length = spec.shape[2]\n",
    "\n",
    "            # --- Padding or Truncating ---\n",
    "            if current_length < target_length:\n",
    "                # Pad with zeros if it's too short\n",
    "                padding_needed = target_length - current_length\n",
    "                # torch.pad takes (data, (pad_left, pad_right, pad_top, pad_bottom, ...))\n",
    "                spec = torch.nn.functional.pad(spec, (0, padding_needed))\n",
    "            elif current_length > target_length:\n",
    "                # Truncate if it's too long\n",
    "                spec = spec[:, :, :target_length]\n",
    "\n",
    "            spectrograms.append(spec)\n",
    "            labels.append(label)\n",
    "\n",
    "        # Stack them into a single batch tensor\n",
    "        spectrograms_batch = torch.cat(spectrograms, dim=0)\n",
    "        labels_batch = torch.tensor(labels)\n",
    "\n",
    "        return spectrograms_batch, labels_batch\n",
    "\n",
    "    # Its job: Define the Transformer architecture.\n",
    "    class AudioTransformer(nn.Module):\n",
    "        def __init__(self, num_input_features=128, num_classes=35, dropout=0.1):\n",
    "            super().__init__()\n",
    "            # Using PyTorch's pre-built Transformer components\n",
    "            encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=num_input_features, nhead=4, batch_first=True, dropout=dropout\n",
    "            )\n",
    "            self.transformer_encoder = nn.TransformerEncoder(\n",
    "                encoder_layer, num_layers=4\n",
    "            )\n",
    "            self.output_layer = nn.Linear(num_input_features, num_classes)\n",
    "\n",
    "        def forward(self, spectrogram_batch):\n",
    "            # Input shape needs to be (batch, time, features) for batch_first=True\n",
    "            # Spectrograms are often (batch, features, time), so we might need to permute\n",
    "            x = spectrogram_batch.permute(0, 2, 1)\n",
    "\n",
    "            x = self.transformer_encoder(x)\n",
    "            x = x.mean(dim=1)  # Average over the time dimension\n",
    "            predictions = self.output_layer(x)\n",
    "            return predictions\n",
    "\n",
    "    class Trainer:\n",
    "        def __init__(\n",
    "            self,\n",
    "            model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            test_loader,\n",
    "            optimizer,\n",
    "            loss_fn,\n",
    "            scheduler,\n",
    "            device,\n",
    "            total_epochs,\n",
    "            exp_path=None,\n",
    "            rank=None,\n",
    "        ):\n",
    "            self.model = model\n",
    "            self.train_loader = train_loader\n",
    "            self.val_loader = val_loader\n",
    "            self.test_loader = test_loader\n",
    "            self.optimizer = optimizer\n",
    "            self.loss_fn = loss_fn\n",
    "            self.scheduler = scheduler\n",
    "            self.device = device\n",
    "            self.total_epochs = total_epochs\n",
    "            self.best_val_accuracy = 0.0\n",
    "            self.step = 0\n",
    "            self.timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "            self.total_steps = len(self.train_loader) * self.total_epochs\n",
    "            if exp_path is None:\n",
    "                self.exp_path = f\"/data/speech-recognition/runs/exp-{self.timestamp}\"\n",
    "            else:\n",
    "                self.exp_path = exp_path\n",
    "            print(f\"Experiment path: {self.exp_path}\")\n",
    "\n",
    "            self.rank = rank\n",
    "            self.is_main_process = self.rank == 0\n",
    "\n",
    "            if self.is_main_process:\n",
    "                self.writer = SummaryWriter(f\"{self.exp_path}/logs\")\n",
    "            else:\n",
    "                self.writer = None\n",
    "\n",
    "        def _train_one_epoch(self, epoch):\n",
    "            self.train_loader.sampler.set_epoch(epoch)\n",
    "            self.model.train()  # Ensure model is in training mode\n",
    "            epoch_loss = 0.0\n",
    "            num_batches = 0\n",
    "\n",
    "            for batch_idx, (spectrograms, labels) in enumerate(self.train_loader):\n",
    "                spectrograms = spectrograms.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                # 1. PREDICT: Pass data through the model\n",
    "                predictions = self.model(spectrograms)\n",
    "\n",
    "                # 2. COMPARE: Calculate the error\n",
    "                loss = self.loss_fn(predictions, labels)\n",
    "\n",
    "                # 3. ADJUST: Update the model's weights\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # Add gradient clipping to prevent explosion\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Update tracking variables\n",
    "                epoch_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                self.step += 1\n",
    "\n",
    "                # Log to TensorBoard every step\n",
    "                if self.is_main_process:\n",
    "                    self.writer.add_scalar(\"Loss/Train\", loss.item(), self.step)\n",
    "                    # Print progress every 10 steps or at the end of each epoch\n",
    "                    if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(\n",
    "                        self.train_loader\n",
    "                    ):\n",
    "                        avg_loss = epoch_loss / num_batches\n",
    "                        print(\n",
    "                            f\"Epoch {epoch+1:2d}/{self.total_epochs} | Step {self.step:4d}/{self.total_steps} | \"\n",
    "                            f\"Batch {batch_idx+1:3d}/{len(self.train_loader)} | \"\n",
    "                            f\"Loss: {loss.item():.6f} | Avg Loss: {avg_loss:.6f}\"\n",
    "                        )\n",
    "\n",
    "            # Log epoch average loss to TensorBoard\n",
    "            avg_epoch_loss = epoch_loss / num_batches\n",
    "            if self.is_main_process:\n",
    "                self.writer.add_scalar(\"Loss/Epoch_Avg\", avg_epoch_loss, epoch + 1)\n",
    "                # Print epoch summary\n",
    "                print(\n",
    "                    f\"Epoch {epoch+1:2d}/{self.total_epochs} completed | Avg Loss: {avg_epoch_loss:.6f}\"\n",
    "                )\n",
    "                print(\"-\" * 80)\n",
    "\n",
    "        def _validate_one_epoch(self, epoch, loader=None):\n",
    "            # Single-machine validation (only runs on rank 0)\n",
    "            self.model.eval()  # 1. Switch to evaluation mode\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            print(\"  Starting validation...\")\n",
    "\n",
    "            with torch.no_grad():  # 2. Do not compute gradients within this code block\n",
    "                for batch_idx, (spectrograms, labels) in enumerate(loader):\n",
    "                    spectrograms = spectrograms.to(self.device)\n",
    "                    labels = labels.to(self.device)\n",
    "\n",
    "                    # Only perform prediction and calculate loss\n",
    "                    predictions = self.model(spectrograms)\n",
    "                    loss = self.loss_fn(predictions, labels)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    # Calculate accuracy\n",
    "                    _, predicted_labels = torch.max(predictions.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted_labels == labels).sum().item()\n",
    "\n",
    "                    # Print validation progress every 10 batches\n",
    "                    if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(loader):\n",
    "                        current_accuracy = 100 * correct / total\n",
    "                        print(\n",
    "                            f\"    Validation Batch {batch_idx+1:3d}/{len(loader)} | \"\n",
    "                            f\"Current Accuracy: {current_accuracy:.2f}%\"\n",
    "                        )\n",
    "\n",
    "            # Simple single-machine calculation\n",
    "            avg_loss = val_loss / len(loader)\n",
    "            val_accuracy = 100 * correct / total\n",
    "\n",
    "            print(\n",
    "                f\"  Validation completed | Loss: {avg_loss:.6f} | Accuracy: {val_accuracy:.2f}%\"\n",
    "            )\n",
    "\n",
    "            if val_accuracy > self.best_val_accuracy:\n",
    "                self.best_val_accuracy = val_accuracy\n",
    "                print(\n",
    "                    f\"New best validation accuracy: {self.best_val_accuracy:.2f}%. Saving model...\"\n",
    "                )\n",
    "                model_folder = f\"{self.exp_path}/models\"\n",
    "                os.makedirs(model_folder, exist_ok=True)\n",
    "                # Handle both DDP and non-DDP models\n",
    "                if hasattr(self.model, \"module\"):\n",
    "                    model_state = self.model.module.state_dict()\n",
    "                else:\n",
    "                    model_state = self.model.state_dict()\n",
    "                torch.save(model_state, f\"{model_folder}/best-epoch{epoch}.pth\")\n",
    "\n",
    "                if self.writer:\n",
    "                    self.writer.add_scalar(\"Loss/Val\", val_loss, epoch + 1)\n",
    "                    self.writer.add_scalar(\"Accuracy/Val\", val_accuracy, epoch + 1)\n",
    "\n",
    "        def train(self):\n",
    "            print(\"Starting training...\")\n",
    "            for epoch in range(self.total_epochs):\n",
    "                self._train_one_epoch(epoch)\n",
    "                # Only validate if val_loader is available (single-machine validation)\n",
    "                if self.val_loader is not None:\n",
    "                    self._validate_one_epoch(epoch, self.val_loader)\n",
    "\n",
    "                # Synchronize all processes after validation\n",
    "                if dist.is_initialized():\n",
    "                    dist.barrier()  # Wait for rank 0 to finish validation\n",
    "\n",
    "                if self.scheduler:\n",
    "                    self.scheduler.step()\n",
    "                if self.is_main_process:\n",
    "                    print(\"-\" * 80)\n",
    "\n",
    "            if self.is_main_process:\n",
    "                self.writer.close()\n",
    "                print(\"Training complete!\")\n",
    "\n",
    "        def test(self):\n",
    "            # Only run test on rank 0 (single-machine testing)\n",
    "            if self.test_loader is None:\n",
    "                return\n",
    "\n",
    "            print(\"Starting test...\")\n",
    "            self.model.eval()  # 1. Switch to evaluation mode\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            with torch.no_grad():  # 2. Do not compute gradients within this code block\n",
    "                for batch_idx, (spectrograms, labels) in enumerate(self.test_loader):\n",
    "                    spectrograms = spectrograms.to(self.device)\n",
    "                    labels = labels.to(self.device)\n",
    "\n",
    "                    # Only perform prediction and calculate loss\n",
    "                    predictions = self.model(spectrograms)\n",
    "                    loss = self.loss_fn(predictions, labels)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    # Calculate accuracy\n",
    "                    _, predicted_labels = torch.max(predictions.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted_labels == labels).sum().item()\n",
    "\n",
    "                    # Print test progress every 10 batches\n",
    "                    if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(self.test_loader):\n",
    "                        current_accuracy = 100 * correct / total\n",
    "                        print(\n",
    "                            f\"    Test Batch {batch_idx+1:3d}/{len(self.test_loader)} | \"\n",
    "                            f\"Current Accuracy: {current_accuracy:.2f}%\"\n",
    "                        )\n",
    "\n",
    "            # Simple single-machine calculation\n",
    "            avg_loss = val_loss / len(self.test_loader)\n",
    "            val_accuracy = 100 * correct / total\n",
    "\n",
    "            print(\n",
    "                f\"  Test completed | Loss: {avg_loss:.6f} | Accuracy: {val_accuracy:.2f}%\"\n",
    "            )\n",
    "            print(\"Test complete!\")\n",
    "\n",
    "    def setup_ddp():\n",
    "        \"\"\"Initialize DDP process group\"\"\"\n",
    "        # This line is key: bind unique GPU for current process\n",
    "        if (\n",
    "            \"LOCAL_RANK\" not in os.environ\n",
    "            or \"RANK\" not in os.environ\n",
    "            or \"WORLD_SIZE\" not in os.environ\n",
    "        ):\n",
    "            print(\"LOCAL_RANK, RANK, and WORLD_SIZE is not set, will skip using DDP\")\n",
    "            return torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\", 0, 0\n",
    "        print(\n",
    "            f\"LOCAL_RANK: {os.environ['LOCAL_RANK']}, RANK: {os.environ['RANK']}, WORLD_SIZE: {os.environ['WORLD_SIZE']}\"\n",
    "        )\n",
    "\n",
    "        local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            dist.init_process_group(backend=\"nccl\")\n",
    "            device = torch.device(\"cuda\", local_rank)\n",
    "            torch.cuda.set_device(device)\n",
    "            print(f\"Using device: {device}\")\n",
    "\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            dist.init_process_group(backend=\"gloo\")\n",
    "\n",
    "        rank = torch.distributed.get_rank()\n",
    "        print(\n",
    "            f\"Rank(dist.get_rank()): {rank}, Rank(os.environ['RANK']): {os.environ['RANK']}, Local Rank(os.environ['LOCAL_RANK']): {local_rank}\"\n",
    "        )\n",
    "\n",
    "        return device, local_rank, rank\n",
    "\n",
    "    def cleanup_ddp():\n",
    "        \"\"\"Destroy process group\"\"\"\n",
    "        dist.destroy_process_group()\n",
    "\n",
    "    def train():\n",
    "        # This line will automatically select GPU (if available), otherwise fall back to CPU\n",
    "        device, local_rank, rank = setup_ddp()\n",
    "\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        # Instantiate the Dataset and DataLoader\n",
    "        print(\"start loading dataset\")\n",
    "        data_path_prefix = \"/data/SpeechCommands/speech_commands_v0.02\"\n",
    "        audio_info, label_map, label_map_reverse = load_data(data_path_prefix)\n",
    "\n",
    "        audio_info_training, audio_info_validation, audio_info_test = split_data(\n",
    "            audio_info\n",
    "        )\n",
    "\n",
    "        if debug:\n",
    "            # Use more data for debug: 4000 train, 500 val, 500 test\n",
    "            audio_info_training = audio_info_training[:4000]\n",
    "            audio_info_validation = audio_info_validation[:500]\n",
    "            audio_info_test = audio_info_test[:500]\n",
    "            print(\n",
    "                f\"Debug mode: using train={len(audio_info_training)}, val={len(audio_info_validation)}, test={len(audio_info_test)}\"\n",
    "            )\n",
    "\n",
    "        train_dataset = SpeechCommandsDataset(\n",
    "            audio_info_training, label_map, label_map_reverse, data_path_prefix\n",
    "        )\n",
    "\n",
    "        val_dataset = SpeechCommandsDataset(\n",
    "            audio_info_validation, label_map, label_map_reverse, data_path_prefix\n",
    "        )\n",
    "        test_dataset = SpeechCommandsDataset(\n",
    "            audio_info_test, label_map, label_map_reverse, data_path_prefix\n",
    "        )\n",
    "\n",
    "        print(\"dataset loaded\")\n",
    "\n",
    "        print(\"start init data loader\")\n",
    "\n",
    "        train_sampler = DistributedSampler(train_dataset, shuffle=True)\n",
    "        # Adjust batch size for debug mode\n",
    "        batch_size = 64 if debug else 256\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=collate_fn_spectrogram,\n",
    "        )\n",
    "        # Use single-machine validation (only rank 0)\n",
    "        if rank == 0:\n",
    "            val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                collate_fn=collate_fn_spectrogram,\n",
    "            )\n",
    "            test_loader = DataLoader(\n",
    "                test_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                collate_fn=collate_fn_spectrogram,\n",
    "            )\n",
    "        else:\n",
    "            val_loader = None\n",
    "            test_loader = None\n",
    "\n",
    "        print(\"data loader initialized\")\n",
    "\n",
    "        # Instantiate the Model, Loss Function, and Optimizer\n",
    "        model = AudioTransformer().to(device)\n",
    "\n",
    "        # Create DDP model - different parameters for CPU vs GPU\n",
    "        if torch.cuda.is_available() and device.type == \"cuda\":\n",
    "            ddp_model = DDP(model, device_ids=[local_rank])\n",
    "        else:\n",
    "            # For CPU training, don't specify device_ids\n",
    "            ddp_model = DDP(model)\n",
    "\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        world_size = dist.get_world_size() if dist.is_initialized() else 1\n",
    "        # Use linear scaling with a more conservative approach\n",
    "        base_lr = 0.001\n",
    "        lr = (\n",
    "            base_lr * min(world_size, 2) if not debug else base_lr\n",
    "        )  # No scaling in debug\n",
    "        print(f\"Using learning rate: {lr} (world_size: {world_size}, debug: {debug})\")\n",
    "        optimizer = torch.optim.Adam(ddp_model.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.9)\n",
    "\n",
    "        # Generate timestamp for this experiment\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        print(f\"Experiment timestamp: {timestamp}\")\n",
    "\n",
    "        # --- 2. Create and start Trainer ---\n",
    "        total_epochs = 10 if debug else 30\n",
    "        trainer = Trainer(\n",
    "            ddp_model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            test_loader,\n",
    "            optimizer,\n",
    "            loss_fn,\n",
    "            scheduler,\n",
    "            device,\n",
    "            total_epochs,\n",
    "            rank=rank,\n",
    "        )\n",
    "        trainer.train()\n",
    "\n",
    "        # --- 3. (Optional) Final test ---\n",
    "        trainer.test()  # You can add a .test() method to Trainer\n",
    "\n",
    "        # Synchronize all processes after test\n",
    "        if dist.is_initialized():\n",
    "            dist.barrier()  # Wait for rank 0 to finish test\n",
    "\n",
    "        cleanup_ddp()\n",
    "\n",
    "        if rank == 0:\n",
    "            print(\"Training complete!\")\n",
    "            print(\"To view TensorBoard, run: tensorboard --logdir=runs\")\n",
    "\n",
    "    train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ccf176-24b0-4dd7-8339-a2e8acdfd76c",
   "metadata": {},
   "source": [
    "## Train in local Jupyter Notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f946b2f6-66d2-4d9b-b039-f2a5cde38357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the Torch Distributed env variables so the training function can be run locally in the Notebook.\n",
    "# See https://pytorch.org/docs/stable/elastic/run.html#environment-variables\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"12345\"\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f999bf7b-857c-4fbf-93c5-83f441bef4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01c4e332-04dc-4dc4-983e-7edb684a0cc2",
   "metadata": {},
   "source": [
    "## Train with Kubeflow Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d7209e1-d45d-4de0-b392-075232427075",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = client.train(\n",
    "    trainer=CustomTrainer(\n",
    "        func=train_model,\n",
    "        # Set how many PyTorch nodes you want to use for distributed training.\n",
    "        num_nodes=2,\n",
    "        # Set the resources for each PyTorch node.\n",
    "        resources_per_node={\n",
    "            \"cpu\": 5,\n",
    "            \"memory\": \"50Gi\",\n",
    "            # Uncomment this to distribute the TrainJob using GPU nodes.\n",
    "            # \"nvidia.com/gpu\": 1,\n",
    "        },\n",
    "    ),\n",
    "    runtime=torch_runtime,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "269cbb92-e30a-4c53-8c9b-750f8e86d1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainJob(name='r646465920c0', creation_timestamp=datetime.datetime(2025, 9, 14, 3, 44, 31, tzinfo=TzInfo(UTC)), runtime=Runtime(name='torch-distributed-speech-recognition', trainer=RuntimeTrainer(trainer_type=<TrainerType.CUSTOM_TRAINER: 'CustomTrainer'>, framework='torch', num_nodes=1, device='Unknown', device_count='Unknown'), pretrained_model=None), steps=[Step(name='node-0', status='Running', pod_name='r646465920c0-node-0-0-qplsf', device='cpu', device_count='5'), Step(name='node-1', status='Running', pod_name='r646465920c0-node-0-1-cbwr5', device='cpu', device_count='5')], num_nodes=2, status='Running')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.wait_for_job_status(name=job_name, status={\"Running\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f60e728e-ab00-4b13-aa52-bc1d61d5b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or use kubectl to get pod and see logs\n",
    "! kubectl get pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd48a91-611a-4891-9be6-b4284ef1b779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
