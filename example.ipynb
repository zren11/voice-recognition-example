{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4e8e0cb-0e4d-4d16-bc3a-8a0f4188b001",
   "metadata": {},
   "source": [
    "# PyTorch DDP Speech Recognition Training Example\n",
    "\n",
    "This example demonstrates how to train a transformer network to classify audio words with Google's [Speech Command](https://huggingface.co/datasets/google/speech_commands). It's a very small dataset that contains words for classification. The dataset is small(2.3G) and it's quite fast to train a small model.\n",
    "\n",
    "This notebook walks you through running that example locally, and how to easily scale PyTorch DDP across multiple nodes with Kubeflow TrainJob."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642915a6",
   "metadata": {},
   "source": [
    "\n",
    "## Prepare the Kubernetes environment using Kind\n",
    "\n",
    "If you already have your own Kubernetes cluster, you can skip this step.\n",
    "\n",
    "For demo purpose, we will create a k8s cluster with [Kind](https://kind.sigs.k8s.io/). In the same folder of this example Jupyter notebook file, there is a Kind file called `kind-config.yaml`. It will create a k8s cluster with 3 workers and /data from host server is mounted to kind k8s cluster server. Therefore you can download data to /data in your local machine and can be accessed from kind cluster as well.\n",
    "\n",
    "To create the kind cluster, run the following command:\n",
    "**Notice** This will create a Kind cluster named 'ml', you only need to run this command once. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7482f7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating cluster \"ml\" ...\n",
      " \u001b[32m✓\u001b[0m Ensuring node image (kindest/node:v1.34.0) 🖼\n",
      " \u001b[32m✓\u001b[0m Preparing nodes 📦 📦 📦 📦 7l\n",
      " \u001b[32m✓\u001b[0m Writing configuration 📜7l\n",
      " \u001b[32m✓\u001b[0m Starting control-plane 🕹️7l\n",
      " \u001b[32m✓\u001b[0m Installing CNI 🔌7l\n",
      " \u001b[32m✓\u001b[0m Installing StorageClass 💾7l\n",
      " \u001b[32m✓\u001b[0m Joining worker nodes 🚜7l\n",
      "Set kubectl context to \"kind-ml\"\n",
      "You can now use your cluster with:\n",
      "\n",
      "kubectl cluster-info --context kind-ml\n",
      "\n",
      "Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community 🙂\n"
     ]
    }
   ],
   "source": [
    "!kind create cluster --name ml --config kind-config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d3b0a",
   "metadata": {},
   "source": [
    "## Add CRD and Kubeflow Trainer operator to Kubernetes cluster\n",
    "\n",
    "The full instruction is at [here](https://www.kubeflow.org/docs/components/trainer/operator-guides/installation/). In short, run this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58964e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace/kubeflow-system serverside-applied\n",
      "\u001b[33;1mWarning:\u001b[0m unrecognized format \"int32\"\n",
      "\u001b[33;1mWarning:\u001b[0m unrecognized format \"int64\"\n",
      "customresourcedefinition.apiextensions.k8s.io/clustertrainingruntimes.trainer.kubeflow.org serverside-applied\n",
      "customresourcedefinition.apiextensions.k8s.io/jobsets.jobset.x-k8s.io serverside-applied\n",
      "customresourcedefinition.apiextensions.k8s.io/trainingruntimes.trainer.kubeflow.org serverside-applied\n",
      "customresourcedefinition.apiextensions.k8s.io/trainjobs.trainer.kubeflow.org serverside-applied\n",
      "serviceaccount/jobset-controller-manager serverside-applied\n",
      "serviceaccount/kubeflow-trainer-controller-manager serverside-applied\n",
      "role.rbac.authorization.k8s.io/jobset-leader-election-role serverside-applied\n",
      "clusterrole.rbac.authorization.k8s.io/jobset-manager-role serverside-applied\n",
      "clusterrole.rbac.authorization.k8s.io/jobset-metrics-reader serverside-applied\n",
      "clusterrole.rbac.authorization.k8s.io/jobset-proxy-role serverside-applied\n",
      "clusterrole.rbac.authorization.k8s.io/kubeflow-trainer-controller-manager serverside-applied\n",
      "rolebinding.rbac.authorization.k8s.io/jobset-leader-election-rolebinding serverside-applied\n",
      "clusterrolebinding.rbac.authorization.k8s.io/jobset-manager-rolebinding serverside-applied\n",
      "clusterrolebinding.rbac.authorization.k8s.io/jobset-metrics-reader-rolebinding serverside-applied\n",
      "clusterrolebinding.rbac.authorization.k8s.io/jobset-proxy-rolebinding serverside-applied\n",
      "clusterrolebinding.rbac.authorization.k8s.io/kubeflow-trainer-controller-manager serverside-applied\n",
      "configmap/jobset-manager-config serverside-applied\n",
      "secret/jobset-webhook-server-cert serverside-applied\n",
      "secret/kubeflow-trainer-webhook-cert serverside-applied\n",
      "service/jobset-controller-manager-metrics-service serverside-applied\n",
      "service/jobset-webhook-service serverside-applied\n",
      "service/kubeflow-trainer-controller-manager serverside-applied\n",
      "deployment.apps/jobset-controller-manager serverside-applied\n",
      "deployment.apps/kubeflow-trainer-controller-manager serverside-applied\n",
      "mutatingwebhookconfiguration.admissionregistration.k8s.io/jobset-mutating-webhook-configuration serverside-applied\n",
      "validatingwebhookconfiguration.admissionregistration.k8s.io/jobset-validating-webhook-configuration serverside-applied\n",
      "validatingwebhookconfiguration.admissionregistration.k8s.io/validator.trainer.kubeflow.org serverside-applied\n"
     ]
    }
   ],
   "source": [
    "!export VERSION=v2.0.0\n",
    "!kubectl apply --server-side -k \"https://github.com/kubeflow/trainer.git/manifests/overlays/manager?ref=${VERSION}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb78ca0",
   "metadata": {},
   "source": [
    "## Prepare Docker Image\n",
    "\n",
    "We need to create a Docker image with requirements.txt, the `Dockerfile` and `requirements.txt` can be found at the same folder of this Jupyter Notebook.\n",
    "\n",
    "To build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0612281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                          docker:default\n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (1/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 197B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.8.0-cuda12.8  0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (1/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 197B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.8.0-cuda12.8  0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (1/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 197B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.8.0-cuda12.8  0.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (1/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 197B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.8.0-cuda12.8  0.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.8s (2/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 197B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/pytorch:2.8.0-cuda12.8  0.7s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.9s (9/9)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 197B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/pytorch:2.8.0-cuda12.8  0.7s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 38B                                           0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/pytorch/pytorch:2.8.0-cuda12.8-cudnn9-devel@sha2  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] RUN apt-get update && apt-get install -y ffmpeg           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/4] COPY requirements.txt .                                   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/4] RUN pip install -r requirements.txt                       0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:f98d06d275aa85a352ca3b4ee886fd7c10a052fef62e4  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/speech-recognition-image:0.1            0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.0s (9/9) FINISHED                                 docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 197B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/pytorch:2.8.0-cuda12.8  0.7s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 38B                                           0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/pytorch/pytorch:2.8.0-cuda12.8-cudnn9-devel@sha2  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] RUN apt-get update && apt-get install -y ffmpeg           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/4] COPY requirements.txt .                                   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/4] RUN pip install -r requirements.txt                       0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:f98d06d275aa85a352ca3b4ee886fd7c10a052fef62e4  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/speech-recognition-image:0.1            0.0s\n",
      "\u001b[0m\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!docker build -t speech-recognition-image:0.1 -f Dockerfile ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0127318f",
   "metadata": {},
   "source": [
    "### Load image to Kind cluster\n",
    "\n",
    "#### Kind cluster\n",
    "\n",
    "If you are using a local Kind cluster, run the following command to load docker image to your local cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dba24342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: \"speech-recognition-image:0.1\" with ID \"sha256:f98d06d275aa85a352ca3b4ee886fd7c10a052fef62e430c01853e6a1cffc689\" not yet present on node \"ml-worker2\", loading...\n",
      "Image: \"speech-recognition-image:0.1\" with ID \"sha256:f98d06d275aa85a352ca3b4ee886fd7c10a052fef62e430c01853e6a1cffc689\" not yet present on node \"ml-worker\", loading...\n",
      "Image: \"speech-recognition-image:0.1\" with ID \"sha256:f98d06d275aa85a352ca3b4ee886fd7c10a052fef62e430c01853e6a1cffc689\" not yet present on node \"ml-worker3\", loading...\n",
      "Image: \"speech-recognition-image:0.1\" with ID \"sha256:f98d06d275aa85a352ca3b4ee886fd7c10a052fef62e430c01853e6a1cffc689\" not yet present on node \"ml-control-plane\", loading...\n"
     ]
    }
   ],
   "source": [
    "!kind load docker-image speech-recognition-image:0.1 --name ml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3725b970",
   "metadata": {},
   "source": [
    "#### Kubernetes cluster\n",
    "\n",
    "If you are not using the local Kind cluster for testing. Please upload the code to your own Docker registry.\n",
    "\n",
    "```bash\n",
    "docker image push <your docker image name with registry info>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d6be3a",
   "metadata": {},
   "source": [
    "## Add Runtime to K8s cluster\n",
    "\n",
    "In the same folder of this Jypyter notebook file, there is a `kubeflow-runtime-example.yaml`. \n",
    "\n",
    "**Please modify the image to the one you just uploaded.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "582d5a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustertrainingruntime.trainer.kubeflow.org/torch-distributed-speech-recognition created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f kubeflow-runtime-example.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2503c397",
   "metadata": {},
   "source": [
    "## Install the Kubeflow SDK\n",
    "\n",
    "You need to install the Kubeflow SDK to interact with Kubeflow Trainer APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f98b1618-bf56-4c23-96d6-1e8332933ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/kubeflow/sdk.git@main\n",
      "  Cloning https://github.com/kubeflow/sdk.git (to revision main) to /tmp/pip-req-build-dhm1y012\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/kubeflow/sdk.git /tmp/pip-req-build-dhm1y012\n",
      "  Resolved https://github.com/kubeflow/sdk.git to commit 6709dcff0f3e68d44b37531d3154829e626f4b62\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: kubeflow-trainer-api>=2.0.0 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from kubeflow==0.1.0) (2.0.0)\n",
      "Requirement already satisfied: kubernetes>=27.2.0 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from kubeflow==0.1.0) (33.1.0)\n",
      "Requirement already satisfied: pydantic>=2.10.0 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from kubeflow==0.1.0) (2.11.7)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow==0.1.0) (2025.8.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow==0.1.0) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow==0.1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow==0.1.0) (6.0.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow==0.1.0) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow==0.1.0) (1.8.0)\n",
      "Requirement already satisfied: requests in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow==0.1.0) (2.32.5)\n",
      "Requirement already satisfied: requests-oauthlib in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow==0.1.0) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow==0.1.0) (3.3.1)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow==0.1.0) (2.5.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from kubernetes>=27.2.0->kubeflow==0.1.0) (0.10)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=27.2.0->kubeflow==0.1.0) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=27.2.0->kubeflow==0.1.0) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=27.2.0->kubeflow==0.1.0) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=27.2.0->kubeflow==0.1.0) (0.6.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from pydantic>=2.10.0->kubeflow==0.1.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from pydantic>=2.10.0->kubeflow==0.1.0) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from pydantic>=2.10.0->kubeflow==0.1.0) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from pydantic>=2.10.0->kubeflow==0.1.0) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from requests->kubernetes>=27.2.0->kubeflow==0.1.0) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sren/miniconda3/envs/py312/lib/python3.12/site-packages (from requests->kubernetes>=27.2.0->kubeflow==0.1.0) (3.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/kubeflow/sdk.git@main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105147ca-c093-457f-99ca-e72f4c0dce93",
   "metadata": {},
   "source": [
    "## Prepare Speech Command Dataset\n",
    "\n",
    "For demo purpose and to simply the process, we are downloading data to /data in the host server. And in the Kind cluster, it's mounting the host's /data folder to cluster's server's /data folder. And in the Kubeflow Runtime, it's mounting the data with hostpath on /data. Therefore everyone is accessing data in /data folder. **Please make sure there is /data folder in the host server.**\n",
    "\n",
    "For other clusters, please create a volume to make sure data can be accessed via /data.\n",
    "\n",
    "The exact path for Speech Command dataset is `/data/SpeechCommands/speech_commands_v0.02`.\n",
    "\n",
    "To download data, run the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ad4906a-415f-4f03-88b4-89c4e84d519c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading SpeechCommands dataset...\n",
      "Download complete!\n",
      "Number of training samples: 105829\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "\n",
    "print(\"Downloading SpeechCommands dataset...\")\n",
    "\n",
    "# This command will download the data to a folder named \"SpeechCommands\"\n",
    "# in your current directory if it's not already there.\n",
    "train_dataset = torchaudio.datasets.SPEECHCOMMANDS(root=\"/data\", download=True)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9ea4c6-40db-4bd1-bae9-e9d5d074d7ac",
   "metadata": {},
   "source": [
    "## Create TrainerClient with Kubeflow Trainer SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89bee197-0a0e-487b-8512-45c4a9ef68f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.trainer import CustomTrainer, TrainerClient\n",
    "\n",
    "client = TrainerClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc07152c-3966-49ee-8377-9f7199467d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.trainer import Runtime, RuntimeTrainer, TrainerType\n",
    "\n",
    "# Create custom Runtime\n",
    "custom_runtime = Runtime(\n",
    "    name=\"custom-pytorch-runtime\",\n",
    "    trainer=RuntimeTrainer(\n",
    "        trainer_type=TrainerType.CUSTOM_TRAINER,\n",
    "        framework=\"torch\",\n",
    "        num_nodes=2,\n",
    "    ),\n",
    "    pretrained_model=None\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b3098d-a90b-46a9-936b-882aee69c049",
   "metadata": {},
   "source": [
    "## Get runtime from K8s cluster\n",
    "\n",
    "After running the below cell, you should see something like the below. If the following cell shows nothing, it mostly because the Custom Kubeflow Runtime is not created. Please go back to previous step to create Kubeflow Runtime with `kubectl`.\n",
    "```\n",
    "Runtime(name='torch-distributed-speech-recognition', trainer=RuntimeTrainer(trainer_type=<TrainerType.CUSTOM_TRAINER: 'CustomTrainer'>, framework='torch', num_nodes=1, device='Unknown', device_count='Unknown'), pretrained_model=None)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "778f9264-9ca6-43f9-afb0-fa00f353f072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime(name='torch-distributed-speech-recognition', trainer=RuntimeTrainer(trainer_type=<TrainerType.CUSTOM_TRAINER: 'CustomTrainer'>, framework='torch', num_nodes=1, device='Unknown', device_count='Unknown'), pretrained_model=None)\n"
     ]
    }
   ],
   "source": [
    "for runtime in client.list_runtimes():\n",
    "    print(runtime)\n",
    "    if runtime.name == \"torch-distributed-speech-recognition\":\n",
    "        torch_runtime = runtime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3853b4e-a97c-4b91-bf73-edb9a0d22508",
   "metadata": {},
   "source": [
    "## Start training\n",
    "\n",
    "The training code is in the `train_with_kubeflow_trainer.py`, which is in the same folder of current Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d7209e1-d45d-4de0-b392-075232427075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import train_with_kubeflow_trainer\n",
    "\n",
    "job_name = client.train(\n",
    "    trainer=CustomTrainer(\n",
    "        func=train_with_kubeflow_trainer.run,\n",
    "        # Set how many PyTorch nodes you want to use for distributed training.\n",
    "        num_nodes=2,\n",
    "        # Set the resources for each PyTorch node.\n",
    "        resources_per_node={\n",
    "            \"cpu\": 2,\n",
    "            \"memory\": \"8Gi\",\n",
    "            # Uncomment this to distribute the TrainJob using GPU nodes.\n",
    "            # \"nvidia.com/gpu\": 1,\n",
    "        },\n",
    "    ),\n",
    "    runtime=torch_runtime,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "269cbb92-e30a-4c53-8c9b-750f8e86d1a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to get TrainJob: default/lfb5b5487cbe",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mApiException\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/kubeflow/trainer/backends/kubernetes/backend.py:313\u001b[39m, in \u001b[36mKubernetesBackend.get_job\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    303\u001b[39m     thread = \u001b[38;5;28mself\u001b[39m.custom_api.get_namespaced_custom_object(\n\u001b[32m    304\u001b[39m         constants.GROUP,\n\u001b[32m    305\u001b[39m         constants.VERSION,\n\u001b[32m   (...)\u001b[39m\u001b[32m    309\u001b[39m         async_req=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    310\u001b[39m     )\n\u001b[32m    312\u001b[39m     trainjob = models.TrainerV1alpha1TrainJob.from_dict(\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m         \u001b[43mthread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDEFAULT_TIMEOUT\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    314\u001b[39m     )\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m multiprocessing.TimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/multiprocessing/pool.py:774\u001b[39m, in \u001b[36mApplyResult.get\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/multiprocessing/pool.py:125\u001b[39m, in \u001b[36mworker\u001b[39m\u001b[34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     result = (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/kubernetes/client/api_client.py:180\u001b[39m, in \u001b[36mApiClient.__call_api\u001b[39m\u001b[34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# perform request and return response\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m response_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[38;5;28mself\u001b[39m.last_response = response_data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/kubernetes/client/api_client.py:373\u001b[39m, in \u001b[36mApiClient.request\u001b[39m\u001b[34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[39m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33mGET\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrest_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGET\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m                                \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m                                \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33mHEAD\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/kubernetes/client/rest.py:244\u001b[39m, in \u001b[36mRESTClientObject.GET\u001b[39m\u001b[34m(self, url, headers, query_params, _preload_content, _request_timeout)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mGET\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, headers=\u001b[38;5;28;01mNone\u001b[39;00m, query_params=\u001b[38;5;28;01mNone\u001b[39;00m, _preload_content=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    243\u001b[39m         _request_timeout=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m                        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m                        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/kubernetes/client/rest.py:238\u001b[39m, in \u001b[36mRESTClientObject.request\u001b[39m\u001b[34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[32m200\u001b[39m <= r.status <= \u001b[32m299\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ApiException(http_resp=r)\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[31mApiException\u001b[39m: (404)\nReason: Not Found\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '10d2ea9a-b684-42af-a777-8f08cf024341', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'X-Kubernetes-Pf-Flowschema-Uid': 'f697160c-79f2-47d7-9cee-de6ae3ec63f2', 'X-Kubernetes-Pf-Prioritylevel-Uid': '3566debf-d21d-4afc-a71b-a0c0a3fbab5f', 'Date': 'Thu, 11 Sep 2025 05:28:21 GMT', 'Content-Length': '254'})\nHTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"trainjobs.trainer.kubeflow.org \\\"lfb5b5487cbe\\\" not found\",\"reason\":\"NotFound\",\"details\":{\"name\":\"lfb5b5487cbe\",\"group\":\"trainer.kubeflow.org\",\"kind\":\"trainjobs\"},\"code\":404}\n\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_for_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRunning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/kubeflow/trainer/api/trainer_client.py:203\u001b[39m, in \u001b[36mTrainerClient.wait_for_job_status\u001b[39m\u001b[34m(self, name, status, timeout, polling_interval)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwait_for_job_status\u001b[39m(\n\u001b[32m    179\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    180\u001b[39m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    183\u001b[39m     polling_interval: \u001b[38;5;28mint\u001b[39m = \u001b[32m2\u001b[39m,\n\u001b[32m    184\u001b[39m ) -> types.TrainJob:\n\u001b[32m    185\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Wait for a TrainJob to reach a desired status.\u001b[39;00m\n\u001b[32m    186\u001b[39m \n\u001b[32m    187\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    201\u001b[39m \u001b[33;03m        TimeoutError: Timeout to wait for TrainJob status.\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_for_job_status\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpolling_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpolling_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/kubeflow/trainer/backends/kubernetes/backend.py:396\u001b[39m, in \u001b[36mKubernetesBackend.wait_for_job_status\u001b[39m\u001b[34m(self, name, status, timeout, polling_interval)\u001b[39m\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    391\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPolling interval \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpolling_interval\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be less than timeout: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    392\u001b[39m     )\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mround\u001b[39m(timeout / polling_interval)):\n\u001b[32m    395\u001b[39m     \u001b[38;5;66;03m# Check the status after event is generated for the TrainJob's Pods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m396\u001b[39m     trainjob = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_job\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    397\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrainJob \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, status \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainjob.status\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    399\u001b[39m     \u001b[38;5;66;03m# Raise an error if TrainJob is Failed and it is not the expected status.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/kubeflow/trainer/backends/kubernetes/backend.py:321\u001b[39m, in \u001b[36mKubernetesBackend.get_job\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[32m    318\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTimeout to get \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstants.TRAINJOB_KIND\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.namespace\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    319\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    322\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to get \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstants.TRAINJOB_KIND\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.namespace\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_trainjob_from_crd(trainjob)\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to get TrainJob: default/lfb5b5487cbe"
     ]
    }
   ],
   "source": [
    "client.wait_for_job_status(name=job_name, status={\"Running\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60e728e-ab00-4b13-aa52-bc1d61d5b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or use kubectl to get pod and see logs\n",
    "! kubectl get pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd48a91-611a-4891-9be6-b4284ef1b779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
