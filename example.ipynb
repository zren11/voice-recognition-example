{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4e8e0cb-0e4d-4d16-bc3a-8a0f4188b001",
   "metadata": {},
   "source": [
    "# PyTorch DDP Speech Recognition Training Example\n",
    "\n",
    "This example demonstrates how to train a transformer network to classify audio words with Google's [Speech Command](https://huggingface.co/datasets/google/speech_commands). It's a very small dataset that contains words for classification. The dataset is small(2.3G) and it's quite fast to train a small model.\n",
    "\n",
    "This notebook walks you through running that example locally, and how to easily scale PyTorch DDP across multiple nodes with Kubeflow TrainJob."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642915a6",
   "metadata": {},
   "source": [
    "\n",
    "## Prepare the Kubernetes environment using Kind\n",
    "\n",
    "If you already have your own Kubernetes cluster, you can skip this step.\n",
    "\n",
    "For demo purpose, we will create a k8s cluster with [Kind](https://kind.sigs.k8s.io/). In the same folder of this example Jupyter notebook file, there is a Kind file called `kind-config.yaml`. It will create a k8s cluster with 3 workers and /data from host server is mounted to kind k8s cluster server. Therefore you can download data to /data in your local machine and can be accessed from kind cluster as well.\n",
    "\n",
    "To create the kind cluster, run the following command:\n",
    "**Notice** This will create a Kind cluster named 'ml', you only need to run this command once. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7482f7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kind create cluster --name ml --config kind-config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d3b0a",
   "metadata": {},
   "source": [
    "## Add CRD and Kubeflow Trainer operator to Kubernetes cluster\n",
    "\n",
    "The full instruction is at [here](https://www.kubeflow.org/docs/components/trainer/operator-guides/installation/). In short, run this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58964e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export VERSION=v2.0.0\n",
    "!kubectl apply --server-side -k \"https://github.com/kubeflow/trainer.git/manifests/overlays/manager?ref=${VERSION}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb78ca0",
   "metadata": {},
   "source": [
    "## Prepare Docker Image\n",
    "\n",
    "We need to create a Docker image with requirements.txt, the `Dockerfile` and `requirements.txt` can be found at the same folder of this Jupyter Notebook.\n",
    "\n",
    "To build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0612281",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -t speech-recognition-image:0.1 -f Dockerfile ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0127318f",
   "metadata": {},
   "source": [
    "### Load image to Kind cluster\n",
    "\n",
    "#### Kind cluster\n",
    "\n",
    "If you are using a local Kind cluster, run the following command to load docker image to your local cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba24342",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kind load docker-image speech-recognition-image:0.1 --name ml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3725b970",
   "metadata": {},
   "source": [
    "#### Kubernetes cluster\n",
    "\n",
    "If you are not using the local Kind cluster for testing. Please upload the code to your own Docker registry.\n",
    "\n",
    "```bash\n",
    "docker image push <your docker image name with registry info>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d6be3a",
   "metadata": {},
   "source": [
    "## Add Runtime to K8s cluster\n",
    "\n",
    "In the same folder of this Jypyter notebook file, there is a `kubeflow-runtime-example.yaml`. \n",
    "\n",
    "**Please modify the image to the one you just uploaded.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582d5a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f kubeflow-runtime-example.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2503c397",
   "metadata": {},
   "source": [
    "## Install the Kubeflow SDK\n",
    "\n",
    "You need to install the Kubeflow SDK to interact with Kubeflow Trainer APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98b1618-bf56-4c23-96d6-1e8332933ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/kubeflow/sdk.git@main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105147ca-c093-457f-99ca-e72f4c0dce93",
   "metadata": {},
   "source": [
    "## Prepare Speech Command Dataset\n",
    "\n",
    "For demo purpose and to simply the process, we are downloading data to /data in the host server. And in the Kind cluster, it's mounting the host's /data folder to cluster's server's /data folder. And in the Kubeflow Runtime, it's mounting the data with hostpath on /data. Therefore everyone is accessing data in /data folder. **Please make sure there is /data folder in the host server.**\n",
    "\n",
    "For other clusters, please create a volume to make sure data can be accessed via /data.\n",
    "\n",
    "The exact path for Speech Command dataset is `/data/SpeechCommands/speech_commands_v0.02`.\n",
    "\n",
    "To download data, run the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad4906a-415f-4f03-88b4-89c4e84d519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "print(\"Downloading SpeechCommands dataset...\")\n",
    "\n",
    "# This command will download the data to a folder named \"SpeechCommands\"\n",
    "# in your current directory if it's not already there.\n",
    "train_dataset = torchaudio.datasets.SPEECHCOMMANDS(root=\"/data\", download=True)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3985ae5-15f6-4c4c-bdc7-828411450dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /data/SpeechCommands/speech_commands_v0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9ea4c6-40db-4bd1-bae9-e9d5d074d7ac",
   "metadata": {},
   "source": [
    "## Create TrainerClient with Kubeflow Trainer SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bee197-0a0e-487b-8512-45c4a9ef68f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.trainer import CustomTrainer, TrainerClient\n",
    "\n",
    "client = TrainerClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b3098d-a90b-46a9-936b-882aee69c049",
   "metadata": {},
   "source": [
    "## Get runtime from K8s cluster\n",
    "\n",
    "After running the below cell, you should see something like the below. If the following cell shows nothing, it mostly because the Custom Kubeflow Runtime is not created. Please go back to previous step to create Kubeflow Runtime with `kubectl`.\n",
    "```\n",
    "Runtime(name='torch-distributed-speech-recognition', trainer=RuntimeTrainer(trainer_type=<TrainerType.CUSTOM_TRAINER: 'CustomTrainer'>, framework='torch', num_nodes=1, device='Unknown', device_count='Unknown'), pretrained_model=None)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778f9264-9ca6-43f9-afb0-fa00f353f072",
   "metadata": {},
   "outputs": [],
   "source": [
    "for runtime in client.list_runtimes():\n",
    "    print(runtime)\n",
    "    if runtime.name == \"torch-distributed-speech-recognition\":\n",
    "        torch_runtime = runtime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3853b4e-a97c-4b91-bf73-edb9a0d22508",
   "metadata": {},
   "source": [
    "## Start training\n",
    "\n",
    "The training code is in the `train_with_kubeflow_trainer.py`, which is in the same folder of current Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f072ad6b-464c-4112-a0ec-a91e6ee2c143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # 1. IMPORTS\n",
    "    # ---\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    import torchaudio\n",
    "    import os  # To navigate file paths\n",
    "    import json\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    import random\n",
    "    from datetime import datetime\n",
    "    import torch.distributed as dist\n",
    "    from torch.utils.data.distributed import DistributedSampler\n",
    "    from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "    debug = False\n",
    "\n",
    "    # Its job: Load an audio file, convert it to a spectrogram, and return it with its numerical label.\n",
    "    def load_data(data_path):\n",
    "        audio_info = []\n",
    "        label_map = {}\n",
    "        label_map_reverse = {}\n",
    "        # Walk through the data directory to find all audio files\n",
    "        # full_data_path = os.path.join(os.path.dirname(__file__), data_path)\n",
    "        full_data_path = data_path\n",
    "\n",
    "        # Get all subdirectories (word labels), excluding special directories\n",
    "        labels = []\n",
    "        for item in os.listdir(full_data_path):\n",
    "            item_path = os.path.join(full_data_path, item)\n",
    "            if (\n",
    "                os.path.isdir(item_path)\n",
    "                and not item.startswith(\"_\")\n",
    "                and item != \"LICENSE\"\n",
    "            ):\n",
    "                labels.append(item)\n",
    "\n",
    "        # Sort labels for consistent mapping\n",
    "        labels.sort()\n",
    "\n",
    "        # Create label to integer mapping\n",
    "        label_map = {label: idx for idx, label in enumerate(labels)}\n",
    "        label_map_reverse = {idx: label for label, idx in enumerate(labels)}\n",
    "\n",
    "        # Collect all audio files\n",
    "        for label in labels:\n",
    "            label_dir = os.path.join(full_data_path, label)\n",
    "            for filename in os.listdir(label_dir):\n",
    "                if filename.endswith(\".wav\"):\n",
    "                    # Store relative path from speech-recognition folder\n",
    "                    relative_path = os.path.join(data_path, label, filename)\n",
    "                    audio_info.append({\"filename\": relative_path, \"label\": label})\n",
    "        return audio_info, label_map, label_map_reverse\n",
    "\n",
    "    def split_data(audio_info):\n",
    "        # Don't shuffle here - let DistributedSampler handle shuffling\n",
    "        # This ensures proper distributed sampling\n",
    "        random.seed(41)\n",
    "        random.shuffle(audio_info)\n",
    "        print(f\"audio info length: {len(audio_info)}\")\n",
    "        train_size = int(len(audio_info) * 0.95)\n",
    "        val_size = int(len(audio_info) * 0.03)\n",
    "        test_size = len(audio_info) - train_size - val_size\n",
    "        print(f\"train size: {train_size}, val size: {val_size}, test size: {test_size}\")\n",
    "\n",
    "        audio_info_training = audio_info[:train_size]\n",
    "        audio_info_validation = audio_info[train_size : train_size + val_size]\n",
    "        audio_info_test = audio_info[train_size + val_size :]\n",
    "        return audio_info_training, audio_info_validation, audio_info_test\n",
    "\n",
    "    class SpeechCommandsDataset(Dataset):\n",
    "        def __init__(\n",
    "            self, audio_info, label_map, label_map_reverse, data_path_prefix=None\n",
    "        ):\n",
    "            self.data_path_prefix = data_path_prefix\n",
    "            self.audio_info = audio_info\n",
    "            self.label_map = label_map\n",
    "            self.label_map_reverse = label_map_reverse\n",
    "            self.transform = torchaudio.transforms.MelSpectrogram(n_mels=128)\n",
    "\n",
    "            print(json.dumps(self.audio_info[0:3], indent=4))\n",
    "            print(self.label_map)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.audio_info)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            audio_info = self.audio_info[index]\n",
    "\n",
    "            # Build full path to audio file\n",
    "            if self.data_path_prefix is None:\n",
    "                audio_path = os.path.join(\n",
    "                    os.path.dirname(__file__), audio_info[\"filename\"]\n",
    "                )\n",
    "            else:\n",
    "                audio_path = os.path.join(self.data_path_prefix, audio_info[\"filename\"])\n",
    "\n",
    "            # Load the audio file\n",
    "            waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "            # Transform to spectrogram\n",
    "            spectrogram = self.transform(waveform)\n",
    "            # print(f\"Spectrogram shape: {spectrogram.shape}\")\n",
    "\n",
    "            # Get the numerical label from the word label\n",
    "            label = self.label_map[audio_info[\"label\"]]\n",
    "\n",
    "            return spectrogram, label\n",
    "\n",
    "    def collate_fn_spectrogram(batch):\n",
    "        # batch is a list of tuples (spectrogram, label)\n",
    "\n",
    "        # Let's set our target length\n",
    "        target_length = 81\n",
    "\n",
    "        spectrograms = []\n",
    "        labels = []\n",
    "\n",
    "        # Loop through each item in the batch\n",
    "        for spec, label in batch:\n",
    "            # spec shape is (1, num_features, time)\n",
    "            current_length = spec.shape[2]\n",
    "\n",
    "            # --- Padding or Truncating ---\n",
    "            if current_length < target_length:\n",
    "                # Pad with zeros if it's too short\n",
    "                padding_needed = target_length - current_length\n",
    "                # torch.pad takes (data, (pad_left, pad_right, pad_top, pad_bottom, ...))\n",
    "                spec = torch.nn.functional.pad(spec, (0, padding_needed))\n",
    "            elif current_length > target_length:\n",
    "                # Truncate if it's too long\n",
    "                spec = spec[:, :, :target_length]\n",
    "\n",
    "            spectrograms.append(spec)\n",
    "            labels.append(label)\n",
    "\n",
    "        # Stack them into a single batch tensor\n",
    "        spectrograms_batch = torch.cat(spectrograms, dim=0)\n",
    "        labels_batch = torch.tensor(labels)\n",
    "\n",
    "        return spectrograms_batch, labels_batch\n",
    "\n",
    "    # Its job: Define the Transformer architecture.\n",
    "    class AudioTransformer(nn.Module):\n",
    "        def __init__(self, num_input_features=128, num_classes=35, dropout=0.1):\n",
    "            super().__init__()\n",
    "            # Using PyTorch's pre-built Transformer components\n",
    "            encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=num_input_features, nhead=4, batch_first=True, dropout=dropout\n",
    "            )\n",
    "            self.transformer_encoder = nn.TransformerEncoder(\n",
    "                encoder_layer, num_layers=4\n",
    "            )\n",
    "            self.output_layer = nn.Linear(num_input_features, num_classes)\n",
    "\n",
    "        def forward(self, spectrogram_batch):\n",
    "            # Input shape needs to be (batch, time, features) for batch_first=True\n",
    "            # Spectrograms are often (batch, features, time), so we might need to permute\n",
    "            x = spectrogram_batch.permute(0, 2, 1)\n",
    "\n",
    "            x = self.transformer_encoder(x)\n",
    "            x = x.mean(dim=1)  # Average over the time dimension\n",
    "            predictions = self.output_layer(x)\n",
    "            return predictions\n",
    "\n",
    "    class Trainer:\n",
    "        def __init__(\n",
    "            self,\n",
    "            model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            test_loader,\n",
    "            optimizer,\n",
    "            loss_fn,\n",
    "            scheduler,\n",
    "            device,\n",
    "            total_epochs,\n",
    "            exp_path=None,\n",
    "            rank=None,\n",
    "        ):\n",
    "            self.model = model\n",
    "            self.train_loader = train_loader\n",
    "            self.val_loader = val_loader\n",
    "            self.test_loader = test_loader\n",
    "            self.optimizer = optimizer\n",
    "            self.loss_fn = loss_fn\n",
    "            self.scheduler = scheduler\n",
    "            self.device = device\n",
    "            self.total_epochs = total_epochs\n",
    "            self.best_val_accuracy = 0.0\n",
    "            self.step = 0\n",
    "            self.timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "            self.total_steps = len(self.train_loader) * self.total_epochs\n",
    "            if exp_path is None:\n",
    "                self.exp_path = f\"/data/speech-recognition/runs/exp-{self.timestamp}\"\n",
    "            else:\n",
    "                self.exp_path = exp_path\n",
    "            print(f\"Experiment path: {self.exp_path}\")\n",
    "\n",
    "            self.rank = rank\n",
    "            self.is_main_process = self.rank == 0\n",
    "\n",
    "            if self.is_main_process:\n",
    "                self.writer = SummaryWriter(f\"{self.exp_path}/logs\")\n",
    "            else:\n",
    "                self.writer = None\n",
    "\n",
    "        def _train_one_epoch(self, epoch):\n",
    "            self.train_loader.sampler.set_epoch(epoch)\n",
    "            self.model.train()  # Ensure model is in training mode\n",
    "            epoch_loss = 0.0\n",
    "            num_batches = 0\n",
    "\n",
    "            for batch_idx, (spectrograms, labels) in enumerate(self.train_loader):\n",
    "                spectrograms = spectrograms.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                # 1. PREDICT: Pass data through the model\n",
    "                predictions = self.model(spectrograms)\n",
    "\n",
    "                # 2. COMPARE: Calculate the error\n",
    "                loss = self.loss_fn(predictions, labels)\n",
    "\n",
    "                # 3. ADJUST: Update the model's weights\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # Add gradient clipping to prevent explosion\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Update tracking variables\n",
    "                epoch_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                self.step += 1\n",
    "\n",
    "                # Log to TensorBoard every step\n",
    "                if self.is_main_process:\n",
    "                    self.writer.add_scalar(\"Loss/Train\", loss.item(), self.step)\n",
    "                    # Print progress every 10 steps or at the end of each epoch\n",
    "                    if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(\n",
    "                        self.train_loader\n",
    "                    ):\n",
    "                        avg_loss = epoch_loss / num_batches\n",
    "                        print(\n",
    "                            f\"Epoch {epoch+1:2d}/{self.total_epochs} | Step {self.step:4d}/{self.total_steps} | \"\n",
    "                            f\"Batch {batch_idx+1:3d}/{len(self.train_loader)} | \"\n",
    "                            f\"Loss: {loss.item():.6f} | Avg Loss: {avg_loss:.6f}\"\n",
    "                        )\n",
    "\n",
    "            # Log epoch average loss to TensorBoard\n",
    "            avg_epoch_loss = epoch_loss / num_batches\n",
    "            if self.is_main_process:\n",
    "                self.writer.add_scalar(\"Loss/Epoch_Avg\", avg_epoch_loss, epoch + 1)\n",
    "                # Print epoch summary\n",
    "                print(\n",
    "                    f\"Epoch {epoch+1:2d}/{self.total_epochs} completed | Avg Loss: {avg_epoch_loss:.6f}\"\n",
    "                )\n",
    "                print(\"-\" * 80)\n",
    "\n",
    "        def _validate_one_epoch(self, epoch, loader=None):\n",
    "            # Single-machine validation (only runs on rank 0)\n",
    "            self.model.eval()  # 1. Switch to evaluation mode\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            print(\"  Starting validation...\")\n",
    "\n",
    "            with torch.no_grad():  # 2. Do not compute gradients within this code block\n",
    "                for batch_idx, (spectrograms, labels) in enumerate(loader):\n",
    "                    spectrograms = spectrograms.to(self.device)\n",
    "                    labels = labels.to(self.device)\n",
    "\n",
    "                    # Only perform prediction and calculate loss\n",
    "                    predictions = self.model(spectrograms)\n",
    "                    loss = self.loss_fn(predictions, labels)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    # Calculate accuracy\n",
    "                    _, predicted_labels = torch.max(predictions.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted_labels == labels).sum().item()\n",
    "\n",
    "                    # Print validation progress every 10 batches\n",
    "                    if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(loader):\n",
    "                        current_accuracy = 100 * correct / total\n",
    "                        print(\n",
    "                            f\"    Validation Batch {batch_idx+1:3d}/{len(loader)} | \"\n",
    "                            f\"Current Accuracy: {current_accuracy:.2f}%\"\n",
    "                        )\n",
    "\n",
    "            # Simple single-machine calculation\n",
    "            avg_loss = val_loss / len(loader)\n",
    "            val_accuracy = 100 * correct / total\n",
    "\n",
    "            print(\n",
    "                f\"  Validation completed | Loss: {avg_loss:.6f} | Accuracy: {val_accuracy:.2f}%\"\n",
    "            )\n",
    "\n",
    "            if val_accuracy > self.best_val_accuracy:\n",
    "                self.best_val_accuracy = val_accuracy\n",
    "                print(\n",
    "                    f\"New best validation accuracy: {self.best_val_accuracy:.2f}%. Saving model...\"\n",
    "                )\n",
    "                model_folder = f\"{self.exp_path}/models\"\n",
    "                os.makedirs(model_folder, exist_ok=True)\n",
    "                # Handle both DDP and non-DDP models\n",
    "                if hasattr(self.model, \"module\"):\n",
    "                    model_state = self.model.module.state_dict()\n",
    "                else:\n",
    "                    model_state = self.model.state_dict()\n",
    "                torch.save(model_state, f\"{model_folder}/best-epoch{epoch}.pth\")\n",
    "\n",
    "                if self.writer:\n",
    "                    self.writer.add_scalar(\"Loss/Val\", val_loss, epoch + 1)\n",
    "                    self.writer.add_scalar(\"Accuracy/Val\", val_accuracy, epoch + 1)\n",
    "\n",
    "        def train(self):\n",
    "            print(\"Starting training...\")\n",
    "            for epoch in range(self.total_epochs):\n",
    "                self._train_one_epoch(epoch)\n",
    "                # Only validate if val_loader is available (single-machine validation)\n",
    "                if self.val_loader is not None:\n",
    "                    self._validate_one_epoch(epoch, self.val_loader)\n",
    "\n",
    "                # Synchronize all processes after validation\n",
    "                if dist.is_initialized():\n",
    "                    dist.barrier()  # Wait for rank 0 to finish validation\n",
    "\n",
    "                if self.scheduler:\n",
    "                    self.scheduler.step()\n",
    "                if self.is_main_process:\n",
    "                    print(\"-\" * 80)\n",
    "\n",
    "            if self.is_main_process:\n",
    "                self.writer.close()\n",
    "                print(\"Training complete!\")\n",
    "\n",
    "        def test(self):\n",
    "            # Only run test on rank 0 (single-machine testing)\n",
    "            if self.test_loader is None:\n",
    "                return\n",
    "\n",
    "            print(\"Starting test...\")\n",
    "            self.model.eval()  # 1. Switch to evaluation mode\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            with torch.no_grad():  # 2. Do not compute gradients within this code block\n",
    "                for batch_idx, (spectrograms, labels) in enumerate(self.test_loader):\n",
    "                    spectrograms = spectrograms.to(self.device)\n",
    "                    labels = labels.to(self.device)\n",
    "\n",
    "                    # Only perform prediction and calculate loss\n",
    "                    predictions = self.model(spectrograms)\n",
    "                    loss = self.loss_fn(predictions, labels)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    # Calculate accuracy\n",
    "                    _, predicted_labels = torch.max(predictions.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted_labels == labels).sum().item()\n",
    "\n",
    "                    # Print test progress every 10 batches\n",
    "                    if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(self.test_loader):\n",
    "                        current_accuracy = 100 * correct / total\n",
    "                        print(\n",
    "                            f\"    Test Batch {batch_idx+1:3d}/{len(self.test_loader)} | \"\n",
    "                            f\"Current Accuracy: {current_accuracy:.2f}%\"\n",
    "                        )\n",
    "\n",
    "            # Simple single-machine calculation\n",
    "            avg_loss = val_loss / len(self.test_loader)\n",
    "            val_accuracy = 100 * correct / total\n",
    "\n",
    "            print(\n",
    "                f\"  Test completed | Loss: {avg_loss:.6f} | Accuracy: {val_accuracy:.2f}%\"\n",
    "            )\n",
    "            print(\"Test complete!\")\n",
    "\n",
    "    def setup_ddp():\n",
    "        \"\"\"Initialize DDP process group\"\"\"\n",
    "        # This line is key: bind unique GPU for current process\n",
    "        if (\n",
    "            \"LOCAL_RANK\" not in os.environ\n",
    "            or \"RANK\" not in os.environ\n",
    "            or \"WORLD_SIZE\" not in os.environ\n",
    "        ):\n",
    "            print(\"LOCAL_RANK, RANK, and WORLD_SIZE is not set, will skip using DDP\")\n",
    "            return torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\", 0, 0\n",
    "        print(\n",
    "            f\"LOCAL_RANK: {os.environ['LOCAL_RANK']}, RANK: {os.environ['RANK']}, WORLD_SIZE: {os.environ['WORLD_SIZE']}\"\n",
    "        )\n",
    "\n",
    "        local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            dist.init_process_group(backend=\"nccl\")\n",
    "            device = torch.device(\"cuda\", local_rank)\n",
    "            torch.cuda.set_device(device)\n",
    "            print(f\"Using device: {device}\")\n",
    "\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            dist.init_process_group(backend=\"gloo\")\n",
    "\n",
    "        rank = torch.distributed.get_rank()\n",
    "        print(\n",
    "            f\"Rank(dist.get_rank()): {rank}, Rank(os.environ['RANK']): {os.environ['RANK']}, Local Rank(os.environ['LOCAL_RANK']): {local_rank}\"\n",
    "        )\n",
    "\n",
    "        return device, local_rank, rank\n",
    "\n",
    "    def cleanup_ddp():\n",
    "        \"\"\"Destroy process group\"\"\"\n",
    "        dist.destroy_process_group()\n",
    "\n",
    "    def train():\n",
    "        # This line will automatically select GPU (if available), otherwise fall back to CPU\n",
    "        device, local_rank, rank = setup_ddp()\n",
    "\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        # Instantiate the Dataset and DataLoader\n",
    "        print(\"start loading dataset\")\n",
    "        data_path_prefix = \"/data/SpeechCommands/speech_commands_v0.02\"\n",
    "        audio_info, label_map, label_map_reverse = load_data(data_path_prefix)\n",
    "\n",
    "        audio_info_training, audio_info_validation, audio_info_test = split_data(\n",
    "            audio_info\n",
    "        )\n",
    "\n",
    "        if debug:\n",
    "            # Use more data for debug: 4000 train, 500 val, 500 test\n",
    "            audio_info_training = audio_info_training[:4000]\n",
    "            audio_info_validation = audio_info_validation[:500]\n",
    "            audio_info_test = audio_info_test[:500]\n",
    "            print(\n",
    "                f\"Debug mode: using train={len(audio_info_training)}, val={len(audio_info_validation)}, test={len(audio_info_test)}\"\n",
    "            )\n",
    "\n",
    "        train_dataset = SpeechCommandsDataset(\n",
    "            audio_info_training, label_map, label_map_reverse, data_path_prefix\n",
    "        )\n",
    "\n",
    "        val_dataset = SpeechCommandsDataset(\n",
    "            audio_info_validation, label_map, label_map_reverse, data_path_prefix\n",
    "        )\n",
    "        test_dataset = SpeechCommandsDataset(\n",
    "            audio_info_test, label_map, label_map_reverse, data_path_prefix\n",
    "        )\n",
    "\n",
    "        print(\"dataset loaded\")\n",
    "\n",
    "        print(\"start init data loader\")\n",
    "\n",
    "        train_sampler = DistributedSampler(train_dataset, shuffle=True)\n",
    "        # Adjust batch size for debug mode\n",
    "        batch_size = 64 if debug else 256\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=collate_fn_spectrogram,\n",
    "        )\n",
    "        # Use single-machine validation (only rank 0)\n",
    "        if rank == 0:\n",
    "            val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                collate_fn=collate_fn_spectrogram,\n",
    "            )\n",
    "            test_loader = DataLoader(\n",
    "                test_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                collate_fn=collate_fn_spectrogram,\n",
    "            )\n",
    "        else:\n",
    "            val_loader = None\n",
    "            test_loader = None\n",
    "\n",
    "        print(\"data loader initialized\")\n",
    "\n",
    "        # Instantiate the Model, Loss Function, and Optimizer\n",
    "        model = AudioTransformer().to(device)\n",
    "\n",
    "        # Create DDP model - different parameters for CPU vs GPU\n",
    "        if torch.cuda.is_available() and device.type == \"cuda\":\n",
    "            ddp_model = DDP(model, device_ids=[local_rank])\n",
    "        else:\n",
    "            # For CPU training, don't specify device_ids\n",
    "            ddp_model = DDP(model)\n",
    "\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        world_size = dist.get_world_size() if dist.is_initialized() else 1\n",
    "        # Use linear scaling with a more conservative approach\n",
    "        base_lr = 0.001\n",
    "        lr = (\n",
    "            base_lr * min(world_size, 2) if not debug else base_lr\n",
    "        )  # No scaling in debug\n",
    "        print(f\"Using learning rate: {lr} (world_size: {world_size}, debug: {debug})\")\n",
    "        optimizer = torch.optim.Adam(ddp_model.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.9)\n",
    "\n",
    "        # Generate timestamp for this experiment\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        print(f\"Experiment timestamp: {timestamp}\")\n",
    "\n",
    "        # --- 2. Create and start Trainer ---\n",
    "        total_epochs = 10 if debug else 30\n",
    "        trainer = Trainer(\n",
    "            ddp_model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            test_loader,\n",
    "            optimizer,\n",
    "            loss_fn,\n",
    "            scheduler,\n",
    "            device,\n",
    "            total_epochs,\n",
    "            rank=rank,\n",
    "        )\n",
    "        trainer.train()\n",
    "\n",
    "        # --- 3. (Optional) Final test ---\n",
    "        trainer.test()  # You can add a .test() method to Trainer\n",
    "\n",
    "        # Synchronize all processes after test\n",
    "        if dist.is_initialized():\n",
    "            dist.barrier()  # Wait for rank 0 to finish test\n",
    "\n",
    "        cleanup_ddp()\n",
    "\n",
    "        if rank == 0:\n",
    "            print(\"Training complete!\")\n",
    "            print(\"To view TensorBoard, run: tensorboard --logdir=runs\")\n",
    "\n",
    "    train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ccf176-24b0-4dd7-8339-a2e8acdfd76c",
   "metadata": {},
   "source": [
    "## Train in local Jupyter Notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f946b2f6-66d2-4d9b-b039-f2a5cde38357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the Torch Distributed env variables so the training function can be run locally in the Notebook.\n",
    "# See https://pytorch.org/docs/stable/elastic/run.html#environment-variables\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"12345\"\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f999bf7b-857c-4fbf-93c5-83f441bef4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01c4e332-04dc-4dc4-983e-7edb684a0cc2",
   "metadata": {},
   "source": [
    "## Train with Kubeflow Trainer\n",
    "\n",
    "### Notice\n",
    "\n",
    "A higher `batch_size` might lead to OOM, you could reduce the `batch_size` above to avoid it.\n",
    "\n",
    "Below in the `resources_per_node`, please change it to a smaller/larger cpu/mem based on real cpu/mem you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7209e1-d45d-4de0-b392-075232427075",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = client.train(\n",
    "    trainer=CustomTrainer(\n",
    "        func=train_model,\n",
    "        # Set how many PyTorch nodes you want to use for distributed training.\n",
    "        num_nodes=2,\n",
    "        # Set the resources for each PyTorch node.\n",
    "        resources_per_node={\n",
    "            \"cpu\": 2,\n",
    "            \"memory\": \"20Gi\",\n",
    "            # Uncomment this to distribute the TrainJob using GPU nodes.\n",
    "            # \"nvidia.com/gpu\": 1,\n",
    "        },\n",
    "    ),\n",
    "    runtime=torch_runtime,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269cbb92-e30a-4c53-8c9b-750f8e86d1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.wait_for_job_status(name=job_name, status={\"Running\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60e728e-ab00-4b13-aa52-bc1d61d5b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or use kubectl to get pod and see logs\n",
    "! kubectl get pod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe9185b-487b-4595-99b7-b35ffeb29db1",
   "metadata": {},
   "source": [
    "## Predict word with trained model\n",
    "\n",
    "Change the `MODEL_PATH` and `DATA_PATH` to match the config you setup earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d33f859-36e8-489e-8722-fda9cc707e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import json\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import glob\n",
    "\n",
    "MODEL_PATH = \"/data/speech-recognition/runs/exp-20250913-205502/models/best-epoch15.pth\"  # Path to your saved best model\n",
    "DATA_PATH = \"/data/SpeechCommands/speech_commands_v0.02\"  # Dataset path\n",
    "\n",
    "\n",
    "class AudioTransformer(nn.Module):\n",
    "    def __init__(self, num_input_features=128, num_classes=35, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Using PyTorch's pre-built Transformer components\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=num_input_features, nhead=4, batch_first=True, dropout=dropout\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, num_layers=4\n",
    "        )\n",
    "        self.output_layer = nn.Linear(num_input_features, num_classes)\n",
    "\n",
    "    def forward(self, spectrogram_batch):\n",
    "        # Input shape needs to be (batch, time, features) for batch_first=True\n",
    "        # Spectrograms are often (batch, features, time), so we might need to permute\n",
    "        x = spectrogram_batch.permute(0, 2, 1)\n",
    "\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1)  # Average over the time dimension\n",
    "        predictions = self.output_layer(x)\n",
    "        return predictions\n",
    "\n",
    "def predict(model, audio_path, label_map_reverse, device):\n",
    "    \"\"\"\n",
    "    Load a WAV file, perform prediction and return results\n",
    "    \"\"\"\n",
    "    # a. Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # b. Prepare data transformation pipeline (must be exactly the same as during training)\n",
    "    transform = torchaudio.transforms.MelSpectrogram(n_mels=128)\n",
    "    target_length = 81\n",
    "\n",
    "    # c. Load and transform audio\n",
    "    waveform, _ = torchaudio.load(audio_path)\n",
    "    spectrogram = transform(waveform)\n",
    "\n",
    "    # d. Pad or truncate to target length (must be exactly the same as during training)\n",
    "    current_length = spectrogram.shape[2]\n",
    "    if current_length < target_length:\n",
    "        padding_needed = target_length - current_length\n",
    "        spectrogram = torch.nn.functional.pad(spectrogram, (0, padding_needed))\n",
    "    elif current_length > target_length:\n",
    "        spectrogram = spectrogram[:, :, :target_length]\n",
    "\n",
    "    # e. Prepare input tensor for model\n",
    "    #    - .unsqueeze(0) adds a dimension at the front to simulate a batch\n",
    "    # input_tensor = spectrogram.unsqueeze(0).to(device)\n",
    "    # New line (correct)\n",
    "    input_tensor = spectrogram.to(device)\n",
    "\n",
    "    # f. Perform prediction (in no_grad context to save computational resources)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_tensor)\n",
    "\n",
    "    # g. Interpret output\n",
    "    #    - Softmax converts model output to probabilities\n",
    "    probabilities = torch.nn.functional.softmax(predictions[0], dim=0)\n",
    "    #    - Argmax finds the index of the class with highest probability\n",
    "    predicted_index = torch.argmax(probabilities).item()\n",
    "    #    - Use label_map_reverse to find corresponding word\n",
    "    predicted_label = label_map_reverse[predicted_index]\n",
    "\n",
    "    return predicted_label, probabilities[predicted_index].item()\n",
    "\n",
    "\n",
    "def get_random_audio_files(data_path, num_files=10):\n",
    "    \"\"\"\n",
    "    Get random audio files from the dataset\n",
    "    \"\"\"\n",
    "    all_audio_files = []\n",
    "\n",
    "    # Collect all wav files from all subdirectories\n",
    "    for label_dir in os.listdir(data_path):\n",
    "        label_path = os.path.join(data_path, label_dir)\n",
    "        if (\n",
    "            os.path.isdir(label_path)\n",
    "            and not label_dir.startswith(\"_\")\n",
    "            and label_dir != \"LICENSE\"\n",
    "        ):\n",
    "            wav_files = glob.glob(os.path.join(label_path, \"*.wav\"))\n",
    "            for wav_file in wav_files:\n",
    "                # Store both file path and true label\n",
    "                all_audio_files.append((wav_file, label_dir))\n",
    "\n",
    "    # Randomly sample files\n",
    "    random.shuffle(all_audio_files)\n",
    "    return all_audio_files[:num_files]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration ---\n",
    "    MODEL_PATH = \"/data/speech-recognition/runs/exp-20250913-205502/models/best-epoch15.pth\"  # Path to your saved best model\n",
    "    DATA_PATH = \"/data/SpeechCommands/speech_commands_v0.02\"  # Dataset path\n",
    "    NUM_SAMPLES = 100  # Number of random audio files to test\n",
    "\n",
    "    # Build label mapping\n",
    "    full_data_path = DATA_PATH\n",
    "    labels = []\n",
    "    for item in os.listdir(full_data_path):\n",
    "        item_path = os.path.join(full_data_path, item)\n",
    "        if os.path.isdir(item_path) and not item.startswith(\"_\") and item != \"LICENSE\":\n",
    "            labels.append(item)\n",
    "\n",
    "    # Sort labels for consistent mapping\n",
    "    labels.sort()\n",
    "    label_map_reverse = {idx: label for idx, label in enumerate(labels)}\n",
    "\n",
    "    # --- Load model ---\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Create model instance and load weights\n",
    "    model = AudioTransformer(num_classes=len(label_map_reverse)).to(device)\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "\n",
    "    print(f\"Model loaded from {MODEL_PATH}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Testing {NUM_SAMPLES} random audio files...\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # --- Get random audio files and run predictions ---\n",
    "    random_files = get_random_audio_files(DATA_PATH, NUM_SAMPLES)\n",
    "\n",
    "    correct_predictions = 0\n",
    "    for i, (audio_path, true_label) in enumerate(random_files, 1):\n",
    "        predicted_word, confidence = predict(\n",
    "            model, audio_path, label_map_reverse, device\n",
    "        )\n",
    "\n",
    "        # Check if prediction is correct\n",
    "        is_correct = predicted_word == true_label\n",
    "        if is_correct:\n",
    "            correct_predictions += 1\n",
    "\n",
    "        status = \"✓\" if is_correct else \"✗\"\n",
    "\n",
    "        print(f\"[{i:2d}/{NUM_SAMPLES}] {status} File: {audio_path}\")\n",
    "        print(\n",
    "            f\"       True: '{true_label}' | Predicted: '{predicted_word}' | Confidence: {confidence:.2%}\"\n",
    "        )\n",
    "        print()\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\n",
    "        f\"Accuracy: {correct_predictions}/{NUM_SAMPLES} ({correct_predictions/NUM_SAMPLES:.1%})\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b4763d-712c-4576-92cb-a40280dbcd5f",
   "metadata": {},
   "source": [
    "Find one wav file from above and compare the prediction and real audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c803292c-5071-4d2b-bcf7-29b1bbe37b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "# Play audio sound\n",
    "Audio(\"/data/SpeechCommands/speech_commands_v0.02/cat/25e95412_nohash_0.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb52ad1-8769-407c-9173-0b26ffcb890b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
